# -*- coding: utf-8 -*-
"""Ai Tools Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pf_NCcGf6gChm6lVpPNV27-w9vPZZW9h
"""

# AI Tools Assignment - Mastering the AI Toolkit
# Notebook-style Python script (use in Google Colab or Jupyter)
# Sections: Part 1 (theory answers), Part 2 (Practical code: Iris Decision Tree, MNIST CNN with TensorFlow, spaCy NER+Sentiment), Part 3 (Ethics + Troubleshooting notes)
# -----------------------------------------------------------------------------



# %% [markdown]
# ## PART 2 — Practical Implementation
# We'll do 3 tasks here:
# 1. Iris dataset with Scikit-learn (Decision Tree)
# 2. MNIST CNN with TensorFlow/Keras (target >95% test accuracy)
# 3. spaCy NER + simple rule-based sentiment on sample Amazon reviews

# -----------------------------------------------------------------------------
# TASK 1: Iris dataset — Decision Tree Classifier (Scikit-learn)
# -----------------------------------------------------------------------------

# %%
# Install packages if running in a fresh Colab environment
# (Uncomment the next lines in Colab if needed)
# !pip install -q scikit-learn pandas matplotlib seaborn

# %%
# Imports for Task 1
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# %%
# Load Iris dataset and create DataFrame
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='species')

df = X.copy()
df['species'] = y

print('Dataset shape:', df.shape)
print(df.head())

# %%
# Quick checks: missing values
print('\nMissing values per column:')
print(df.isnull().sum())

# Note: Iris has no missing values in the classic dataset. If there were, we would impute.

# %%
# Encode labels (they are numeric already in this dataset) and split
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
# y_encoded = le.fit_transform(df['species']) # species already numeric
X = df[iris.feature_names]
y = df['species']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)

# %%
# Train Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# %%
# Evaluate
y_pred = dt.predict(X_test)
acc = accuracy_score(y_test, y_pred)
prec_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
rec_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)

print(f'Accuracy: {acc:.4f}')
print(f'Precision (macro): {prec_macro:.4f}')
print(f'Recall (macro): {rec_macro:.4f}\n')
print('Classification report:\n', classification_report(y_test, y_pred, target_names=iris.target_names))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Iris - Confusion Matrix')
plt.show()

# %%
# Plot the trained decision tree (small)
plt.figure(figsize=(12,8))
plot_tree(dt, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True)
plt.title('Decision Tree for Iris')
plt.show()

# -----------------------------------------------------------------------------
# TASK 2: MNIST Handwritten Digits — CNN with TensorFlow/Keras
# -----------------------------------------------------------------------------

# %%
# Install TensorFlow in Colab if necessary (uncomment in Colab)
# !pip install -q tensorflow matplotlib

# %%
# Imports for TensorFlow task
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
import random

print('TensorFlow version:', tf.__version__)

# %%
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
print('Original shapes:', x_train.shape, y_train.shape, x_test.shape, y_test.shape)

# %%
# Preprocess: normalize, add channel dimension
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Expand dims for channel
x_train = np.expand_dims(x_train, -1)  # shape (N, 28,28,1)
x_test = np.expand_dims(x_test, -1)

# One-hot encode labels for training (using sparse_categorical_crossentropy not necessary but we'll keep labels as ints for metrics)
num_classes = 10

# %%
# Build CNN model
def make_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

model = make_model()
model.summary()

# %%
# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# %%
# Callbacks: EarlyStopping and ModelCheckpoint (to keep best weights)
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),
]

# %%
# Train model — small number of epochs first; MNIST converges quickly
history = model.fit(x_train, y_train, epochs=12, batch_size=128, validation_split=0.1, callbacks=callbacks)

# %%
# Plot training history
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# %%
# Evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f'Test accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}')

# If accuracy is below 0.95, train a bit more (but beware of time limits)
if test_acc < 0.95:
    print('\nTest accuracy below 0.95 — training for additional epochs...')
    extra_history = model.fit(x_train, y_train, epochs=6, batch_size=128, validation_split=0.1, callbacks=callbacks)
    # Re-evaluate
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
    print(f'New test accuracy: {test_acc:.4f}')

# %%
# Show predictions on 5 random test images
indices = random.sample(range(x_test.shape[0]), 5)
sample_images = x_test[indices]
sample_labels = y_test[indices]
preds = model.predict(sample_images)
pred_classes = np.argmax(preds, axis=1)

plt.figure(figsize=(12,3))
for i, idx in enumerate(indices):
    plt.subplot(1,5,i+1)
    plt.imshow(sample_images[i].reshape(28,28), cmap='gray')
    plt.title(f'True:{sample_labels[i]}\nPred:{pred_classes[i]}')
    plt.axis('off')
plt.suptitle('MNIST sample predictions')
plt.show()

# Save model (optional)
# model.save('mnist_cnn_model')

# -----------------------------------------------------------------------------
# TASK 3: NLP with spaCy — NER + rule-based sentiment on sample Amazon reviews
# -----------------------------------------------------------------------------

# %%
# Install spaCy and the English model if needed (uncomment in Colab):
# !pip install -q spacy
# !python -m spacy download en_core_web_sm

# %%
import spacy

try:
    nlp = spacy.load('en_core_web_sm')
except Exception as e:
    print('en_core_web_sm model not found. You may need to run: python -m spacy download en_core_web_sm')
    raise

# %%
# Sample Amazon-style reviews (you can replace with actual dataset samples)
reviews = [
    "I bought the Acme SuperVac 3000 last week. The suction is amazing and the battery life is great. Highly recommend!",
    "Terrible experience. My new ZetaPhone speaker stopped working after two days. Customer service was unhelpful.",
    "The LuxComfort pillow is comfortable but the cover started tearing after one wash. I'm disappointed.",
    "Great value for the price — the QuickCharge 2.0 charger from FastCharge works perfectly with my phone.",
    "Not as described. The product (Zeta Earbuds) looked fake and sound quality is poor."
]

# %%
# Simple rule-based sentiment lexicon (very small) — expand for better coverage
positive_words = set(['great', 'amazing', 'good', 'perfect', 'recommend', 'comfortable', 'works', 'excellent'])
negative_words = set(['terrible', 'stopped', 'unhelpful', 'disappointed', 'poor', 'not', 'fake'])

# %%
# Process reviews with spaCy, extract entities and simple sentiment
results = []
for text in reviews:
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    # Determine product/brand mentions heuristically: look for ORG or PRODUCT or capitalized tokens
    product_brand_candidates = [ent.text for ent in doc.ents if ent.label_ in ('ORG','PRODUCT','PERSON')]
    # Simple sentiment scoring by counting lexicon matches
    words = [token.text.lower() for token in doc]
    pos_count = sum(1 for w in words if w in positive_words)
    neg_count = sum(1 for w in words if w in negative_words)
    if pos_count > neg_count:
        sentiment = 'Positive'
    elif neg_count > pos_count:
        sentiment = 'Negative'
    else:
        sentiment = 'Neutral'
    results.append({'text': text, 'entities': entities, 'product_brand_candidates': product_brand_candidates, 'sentiment': sentiment})

# %%
# Display results
for r in results:
    print('Review: ', r['text'])
    print('Extracted entities:', r['entities'])
    print('Product/brand candidates (heuristic):', r['product_brand_candidates'])
    print('Sentiment (rule-based):', r['sentiment'])
    print('-'*80)